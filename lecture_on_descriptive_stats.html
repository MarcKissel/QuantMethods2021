<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Descriptive statistics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Marc Kissel" />
    <meta name="date" content="2021-03-01" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Descriptive statistics
### Marc Kissel
### 2021-03-01

---







---
# Today's class: Descriptive statistics

##Goal is to summarise data in easily understood fashion



```r
#install.packages("lsr")
#install.packages("psych")
library(lsr)
library(tidyverse)
library(psych)
load("aflsmall.Rdata")
```

https://tomfaulkenberry.github.io/JASPbook/chapters/chapter4.pdf   

???
afl.margins variable contains
the winning margin (number of points) for all 176 home and away games played during the 2010 season.
The afl.finalists variable contains the names of all 400 teams that played in all 200 finals matches
played during the period 1987 to 2010. Let's have a look at the afl.margins variable:

---



```r
print(afl.margins)
```

```
##   [1]  56  31  56   8  32  14  36  56  19   1   3 104  43
##  [14]  44  72   9  28  25  27  55  20  16  16   7  23  40
##  [27]  48  64  22  55  95  15  49  52  50  10  65  12  39
##  [40]  36   3  26  23  20  43 108  53  38   4   8   3  13
##  [53]  66  67  50  61  36  38  29   9  81   3  26  12  36
##  [66]  37  70   1  35  12  50  35   9  54  47   8  47   2
##  [79]  29  61  38  41  23  24   1   9  11  10  29  47  71
##  [92]  38  49  65  18   0  16   9  19  36  60  24  25  44
## [105]  55   3  57  83  84  35   4  35  26  22   2  14  19
## [118]  30  19  68  11  75  48  32  36  39  50  11   0  63
## [131]  82  26   3  82  73  19  33  48   8  10  53  20  71
## [144]  75  76  54  44   5  22  94  29   8  98   9  89   1
## [157] 101   7  21  52  42  21 116   3  44  29  27  16   6
## [170]  44   3  28  38  29  10  10
```
##point is that the output isn't very useful. we can't just look at data and figure out what is happening


---
##Sample vs. population 

- A population is the entire group that you want to draw conclusions about, while the  sample is the specific group that you will collect data from.

- i.e. the population can be the undergrads in the US but the sample might be 150 students in a GenEd course at AppState

- Population: scores on an exam in a class


###Note
- *parameter* is a measure that describes the whole population. 

- A *statistic* is a measure that describes the sample.

---

![](https://statsandr.com/blog/what-is-the-difference-between-population-and-sample_files/population-sample.png)

---
#Sampling error

- A sampling error is the difference between a population parameter and a sample statistic. For example, the sampling error is the difference between the mean political attitude rating of your sample and the true mean political attitude rating of all undergraduate students





---
###An average is the value that best represents an entire group

###Central Tendency
______________________

##what is the **middle** of your data?

#1. mean
#2. median
#3. mode



---
# The Mean

### this is the average as we learned in math class. Take all the values and divide  by the total number of values


???
N = number of observations
$$X_1=  first observation $$

`$$x_{n} = {N}th observation$$`


`$$\sum$$`


`$$\sum_{i=1}^{9} X_i$$`

"the sum, taken over all i values from 1 to 9, of the value Xi". 

---
mean formula


## `$$\overline{X} = \frac{1}{n}  \sum_{i=1}^{N} X_i$$`

## maybe  review for many of you, but these symbols are impt later on



---
#how to do mean in R??

## lets say we have 4 values: 34,78,56,35
##the long way


```r
(34 + 78 + 56+35)/4
```

```
## [1] 50.75
```

---

#or

```r
a &lt;- c(34, 78, 56, 35)
sum(a)/length(a)
```

```
## [1] 50.75
```

---
#practice

---
# or, just use the mean function!


```r
mean(x = a)
```

```
## [1] 50.75
```

```r
mean(a)
```

```
## [1] 50.75
```

```r
#mean
```

look at the help on mean function. how do you deal with missing values?

---
#Median -the middle value of a set of observations


##12,15,19,21,23
---
#Median -the middle value of a set of observations

##the middle value of a set of observations


##12,15,**19**,21,23

----
#what about now?

##12,15,19,20,21,23

---
#again, pretty easy in R


```r
print(a)
```

```
## [1] 34 78 56 35
```

```r
sort(a)
```

```
## [1] 34 35 56 78
```

```r
mean(x =c(35,56))
```

```
## [1] 45.5
```



```r
median(a)
```

```
## [1] 45.5
```

---
![](images/mean_median.png)

###mean = center of gravity
###median = middle value

???
An illustration of the difference between how the mean and the median should be interpreted.
The mean is basically the \centre of gravity" of the data set: if you imagine that the histogram of the
data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean.
In contrast, the median is the middle observation. Half of the observations are smaller, and half of the
observations are larger.

---
# some tips

###If data are **nominal (categorical)** then prob not use mean or median

##For income, we tend to use the median rather than the mean.
###*why?*

http://www.abc.net.au/news/stories/2010/09/24/3021480.htm

---
#the trimmed mean

##what do you do  with a *messy* dataset?

-100; 2; 3; 4; 5; 6; 7; 8; 9; 10

-15; 2; 3; 4; 5; 6; 7; 8; 9; 12

##what is the mean of  these? 

---
#Problem is the mean is not *robust*; it is highly influenced by extreme values

##trimmed mean discards a percentage of the observations

0% trimmed mean is just the regular mean, and
the 50% trimmed mean is the median.

---
lets practice 


```r
dataset &lt;- c( -15,2,3,4,5,6,7,8,9,12 )
```


```r
mean(dataset)
```

```
## [1] 4.1
```

```r
median(dataset)
```

```
## [1] 5.5
```

#lets try a 10%  trimmed mean


```r
mean( x = dataset, trim = .1)
```

```
## [1] 5.5
```



---
#mode

##value that occurs most frequently.


```r
table( afl.finalists )
```

```
## afl.finalists
##         Adelaide         Brisbane          Carlton 
##               26               25               26 
##      Collingwood         Essendon          Fitzroy 
##               28               32                0 
##        Fremantle          Geelong         Hawthorn 
##                6               39               27 
##        Melbourne  North Melbourne    Port Adelaide 
##               28               28               17 
##         Richmond         St Kilda           Sydney 
##                6               24               26 
##       West Coast Western Bulldogs 
##               38               24
```


#which is the mode?

---


```r
mode(afl.finalists)
```

```
## [1] "numeric"
```

```r
#um, wtf?
```



```r
#library(lsr)
modeOf(afl.finalists)
```

```
## [1] "Geelong"
```



---
# Variability


```r
group_1 &lt;- c(7,6,3,3,1)
group_2 &lt;- c(3,4,4,5,4)
group_3 &lt;- c(4,4,4,4,4)
```

### - these all have same mean but there is something different about them all. Variability tells us how different scores are from each other. 

### - in theory we can ask how different a series of numbers are from *any* number in the group, but it sorta makes sense to pick the mean
---
# Measures of Variability
________________________________

##How spread out are the data?

###1. Range - the bigest value minus the smallest
###2. Interquartile range
###3. Mean absolute deviation
###4. Variance
###5. Standard deviation



---
##Interquartile range

###calculates the difference between the 25th quantile &amp; the 75th quantile. fun stats fact: The median is the 50% quantile


```r
quantile(afl.margins)
```

```
##     0%    25%    50%    75%   100% 
##   0.00  12.75  30.50  50.50 116.00
```

```r
quantile(afl.margins, probs= .5)
```

```
##  50% 
## 30.5
```


###how would you get the 25% and 75% 

---


```r
IQR(afl.margins)
```

```
## [1] 37.75
```

---



background-image: url(https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/250px-Boxplot_vs_PDF.svg.png)
background-size: contain


???

The
simplest way to think about it is like this: the interquartile range is the range spanned by the \middle
half" of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the
data is above the 75th percentile, leaving the \middle half" of the data lying in between the two. And
the IQR is the range covered by that middle half.

---
#Deviations


##these work by selecting a reference point and report a **typical** deviation from that

---
###Mean absolute deviation - steps:

####1. 56, 31, 56, 8, 32. &lt;- mean = 36.6

####2. calcuate how much each value deviates  from the mean and take the absolute value
      56 - 36.6 = 19.4.
      31- 36.6  = 5.6
      56 - 36.6 = 19.4
      8 - 36.6 = 28.6
      32 - 36.6 = 4.6

####3. then get the mean of these devations. that equals the mean absolute deviation


```r
d &lt;- c(19.4,5.6,19.4,28.6,4.6)
mean(d)
```

```
## [1] 15.52
```



---
#Variance

very similar to mean absolute deviation, but instead we square the deviation rather than using the absolute deviation.

## `$$s^2$$`

## `$$var({X}) = \frac{1}{n}  \sum_{i=1}^{N} (X_i - \overline{x})^2$$`


---
Find the variance of this dataset


### 4, 8, 15, 16, 23, 42


```r
set &lt;- c(4, 8, 15, 16, 23, 42)
#formula
(sum((set - mean(set))^2))/6
```

```
## [1] 151.6667
```

```r
#function
var(set)
```

```
## [1] 182
```


results are different!!!


n vs. n-1

???

However, as we'll discuss in Chapter 10, there's a subtle distinction
between \describing a sample" and \making guesses about the population from which the sample came".
Up to this point, it's been a distinction without a dierence. Regardless of whether you're describing a
sample or drawing inferences about the population, the mean is calculated exactly the same way. Not
so for the variance, or the standard deviation, or for many other measures besides. What I outlined to
you initially (i.e., take the actual average, and thus divide by N) assumes that you literally intend to
calculate the variance of the sample. Most of the time, however, you're not terribly interested in the
sample in and of itself. Rather, the sample exists to tell you something about the world. If so, you're
actually starting to move away from calculating a \sample statistic", and towards the idea of estimating
a \population parameter". However, I'm getting ahead of myself. For now, let's just take it on faith
that R knows what it's doing, and we'll revisit the question later on when we talk about estimation in
Chapter 10.

---
#Problems with Variance 

## it isn't easy to interpert. All the numbers are squared..

## let's say i have thevariance of some measure of how tall a hominin speices was. what is the meaining  ofsaying the variance is 256 meters-squared

##gibberish units...but useful in mathy terms!


---
#standard deviation 

- The standard deviation pretty much lets us convert variance into *meaningful terms*
- In fact, sometimes it is called the root mean squared deviation RMSD


`$$s  = \sqrt{  \frac{1}{n}  \sum_{i=1}^{N} (X_i - \overline{x})^2}$$`


???

Because the standard deviation is derived
from the variance, and the variance is a quantity that has little to no meaning that makes sense to us
humans, the standard deviation doesn't have a simple interpretation. As a consequence, most of us just
rely on a simple rule of thumb: in general, you should expect 68% of the data to fall within 1 standard
deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of
the data to fall within 3 standard deviations of the mean. This rule tends to work pretty well most
of the time, but it's not exact: it's actually calculated based on an assumption that the histogram is
symmetric and bell shaped".10 As

---

background-image: url(https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/1200px-Standard_deviation_diagram.svg.png)
background-size: contain

####in general, you should expect 68% of the data to fall within 1 standard deviation of the mean, 95% of the data to fall within 2 standard deviation of the mean, and 99.7% of  the data to fall within 3 standard deviations

---
# summary

- SD and variance both tell us something about the 'spread' of the data
- variance is helpful since it makes some math stuff easier!
- SD, though, is stated in the original units






---
Next time...
correlation etc

---


background-image: url(https://s3.amazonaws.com/libapps/accounts/73082/images/Skeweness.jpg)
background-size: contain




???
![](http://jse.amstat.org/v13n2/vonhippel.html)

???
if the data tend to have a lot of extreme small values (i.e., the lower tail is \longer"
than the upper tail) and not so many extremely large values (left panel), then we say that the data are
negatively skewed. On the other hand, if there are more extremely large values than extremely small ones
(right panel) we say that the data are positively skewed. That's the qualitative idea behind skewness.
The actual formula for the skewness of a data set is as follows


Many textbooks teach a rule of thumb stating that the mean is right of the median under right skew, and left of the median under left skew. This rule fails with surprising frequency. It can fail in multimodal distributions, or in distributions where one tail is long but the other is heavy. Most commonly, though, the rule fails in discrete distributions where the areas to the left and right of the median are not equal. Such distributions not only contradict the textbook relationship between mean, median, and skew, they also contradict the textbook interpretation of the median. We discuss ways to correct ideas about mean, median, and skew, while enhancing the desired intuition.
---


background-image: url(https://i.stack.imgur.com/KBQLN.jpg)
background-size: contain

#Kurtosis - the 'pointeness' of the data




---

---
# R tips

### summary function 


```r
summary(diamonds)
```

```
##      carat               cut        color    
##  Min.   :0.2000   Fair     : 1610   D: 6775  
##  1st Qu.:0.4000   Good     : 4906   E: 9797  
##  Median :0.7000   Very Good:12082   F: 9542  
##  Mean   :0.7979   Premium  :13791   G:11292  
##  3rd Qu.:1.0400   Ideal    :21551   H: 8304  
##  Max.   :5.0100                     I: 5422  
##                                     J: 2808  
##     clarity          depth           table      
##  SI1    :13065   Min.   :43.00   Min.   :43.00  
##  VS2    :12258   1st Qu.:61.00   1st Qu.:56.00  
##  SI2    : 9194   Median :61.80   Median :57.00  
##  VS1    : 8171   Mean   :61.75   Mean   :57.46  
##  VVS2   : 5066   3rd Qu.:62.50   3rd Qu.:59.00  
##  VVS1   : 3655   Max.   :79.00   Max.   :95.00  
##  (Other): 2531                                  
##      price             x                y         
##  Min.   :  326   Min.   : 0.000   Min.   : 0.000  
##  1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720  
##  Median : 2401   Median : 5.700   Median : 5.710  
##  Mean   : 3933   Mean   : 5.731   Mean   : 5.735  
##  3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540  
##  Max.   :18823   Max.   :10.740   Max.   :58.900  
##                                                   
##        z         
##  Min.   : 0.000  
##  1st Qu.: 2.910  
##  Median : 3.530  
##  Mean   : 3.539  
##  3rd Qu.: 4.040  
##  Max.   :31.800  
## 
```

### describe function
needs the psych package. do you remember how to get a package/


```r
#describe(diamonds)
```


---
#Correlations -Describe the relationships between variables in the data.

##covariance 

`$$Cov(X,Y)  = \  \frac{1}{n-1}  \sum_{i=1}^{N} (X_i - \overline{X})(Y_i - \overline{Y})$$`

if 
#### Cov(X,Y) = 0, then unrealted
#### Cov(X,Y) = +, then positive relationship
#### Cov(X,Y) = -, then negative relationship

But, note what units we are in.....

???
The covariance between two variables X and Y is a generalisation of the notion of the
variance; it's a mathematically simple way of describing the relationship between two variables that isn't
terribly informative to humans:

Because we're multiplying (i.e., taking the \product" of) a quantity that depends on X by a quantity
that depends on Y and then averaging17, you can think of the formula for the covariance as an \average
cross product" between X and Y . The covariance has the nice property that, if X and Y are entirely
unrelated, then the covariance is exactly zero. If the relationship between them is positive (in the
sense shown in Figure 5.9) then the covariance is also positive; and if the relationship is negative then
the covariance is also negative. In other words, the covariance captures the basic qualitative idea of
correlation. Unfortunately, the raw magnitude of the covariance isn't easy to interpret: it depends on
the units in which X and Y are expressed, and worse yet, the actual units that the covariance itself
is expressed in are really weird. For instance, if X refers to the dan.sleep variable (units: hours) and
Y refers to the dan.grump variable (units: grumps), then the units for their covariance are \hours 
grumps". And I have no freaking idea what that would even mean.


---

#Pearson correlation coefficient - *r*
______________________________________

- This *fixes* covaraince by standardizing it.
- Measures the strength of the **linear** relationship between two variables. 

`$$r_{xy}  =  \frac{Cov(X,Y)}{\sigma_x \sigma_y}$$`

- between -1 (perfect negative relationship) and 1 a perfect positive relationship.

gives a measure of the extent to which the data all tend to
fall on a single, perfectly straight line.

also tells us how knowing one value gives us info on the other value



???

However, because we have two variables that contribute to the covariance, the standardisation
only works if we divide by both standard deviations.18 In other words, the correlation between X and Y
can be written as follows:
rXY 
CovpX; Y q
^X ^Y
By doing this standardisation, not only do we keep all of the nice properties of the covariance discussed
earlier, but the actual values of r are on a meaningful scale: r  1 implies a perfect positive relationship,
and r  1 implies a perfect negative relationship. I'll expand a little more on this point later, in
Section 5.7.5. But before I do, let's look at how to calculate correlations in R

---
#Examples of covariance  in R



```r
#cov(mtcars)
```

---
#Examples of correlation  in R


```r
#cor(mtcars)
```


---
#Corrgrams

exploratory visual display and  depicting the patterns of relations among variables 

#need the corrgram package for this

![](lecture_on_descriptive_stats_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;


--

```r
anscombe.1 &lt;- data.frame(x = anscombe[["x1"]], y = anscombe[["y1"]], Set = "Anscombe Set 1")
anscombe.2 &lt;- data.frame(x = anscombe[["x2"]], y = anscombe[["y2"]], Set = "Anscombe Set 2")
anscombe.3 &lt;- data.frame(x = anscombe[["x3"]], y = anscombe[["y3"]], Set = "Anscombe Set 3")
anscombe.4 &lt;- data.frame(x = anscombe[["x4"]], y = anscombe[["y4"]], Set = "Anscombe Set 4")

anscombe.1 %&gt;% ggplot(aes(x=x,y=y)) + geom_point()
```

![](lecture_on_descriptive_stats_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;

```r
anscombe.2 %&gt;% ggplot(aes(x=x,y=y)) + geom_point()
```

![](lecture_on_descriptive_stats_files/figure-html/unnamed-chunk-23-2.png)&lt;!-- --&gt;

```r
anscombe.3 %&gt;% ggplot(aes(x=x,y=y)) + geom_point()
```

![](lecture_on_descriptive_stats_files/figure-html/unnamed-chunk-23-3.png)&lt;!-- --&gt;

```r
anscombe.4 %&gt;% ggplot(aes(x=x,y=y)) + geom_point()
```

![](lecture_on_descriptive_stats_files/figure-html/unnamed-chunk-23-4.png)&lt;!-- --&gt;



---

watch hans roling and then...

![](https://speakerdeck.com/hadley/welcome-to-the-tidyverse)


---


#R practice



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
